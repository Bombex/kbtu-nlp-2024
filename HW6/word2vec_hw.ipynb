{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import bigrams, trigrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from collections import Counter, defaultdict\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/akeresh/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/akeresh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/akeresh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('gutenberg')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_punctuation = string.punctuation + \"''--\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = list(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text dataset Edgeworth Parents.:\n",
    "edgeworth = nltk.corpus.gutenberg.words('edgeworth-parents.txt')\n",
    "#filter from punctuation:\n",
    "edgeworth_filt = [token for token in edgeworth if token not in updated_punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210663, 174879)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word count\n",
    "len(edgeworth), len(edgeworth_filt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgeworth_tokens =  word_tokenize(\" \".join(edgeworth_filt).lower())\n",
    "clean_edgeworth_tokens = [token for token in edgeworth_tokens if token not in stop_words and token not in updated_punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(179099, 78183)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Token count\n",
    "len(edgeworth_tokens), len(clean_edgeworth_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create bigrams, and calculate frequency, conditional frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgeworth_bigram_freq = Counter(bigrams(clean_edgeworth_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('parent', 'assistant'), 1),\n",
       " (('assistant', 'maria'), 1),\n",
       " (('maria', 'edgeworth'), 1),\n",
       " (('edgeworth', 'orphans'), 1),\n",
       " (('orphans', 'near'), 1),\n",
       " (('near', 'ruins'), 1),\n",
       " (('ruins', 'castle'), 1),\n",
       " (('castle', 'rossmore'), 4),\n",
       " (('rossmore', 'ireland'), 1),\n",
       " (('ireland', 'small'), 1)]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(edgeworth_bigram_freq.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_counts_for_w1 = defaultdict(int)\n",
    "for (w1, w2), freq in edgeworth_bigram_freq.items():\n",
    "    total_counts_for_w1[w1] += freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('parent', 1),\n",
       " ('assistant', 1),\n",
       " ('maria', 1),\n",
       " ('edgeworth', 1),\n",
       " ('orphans', 21),\n",
       " ('near', 53),\n",
       " ('ruins', 12),\n",
       " ('castle', 35),\n",
       " ('rossmore', 12),\n",
       " ('ireland', 10)]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(total_counts_for_w1.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63855/63855 [00:00<00:00, 1365532.15it/s]\n"
     ]
    }
   ],
   "source": [
    "bigram_conditional_freq = defaultdict(lambda: defaultdict(float))\n",
    "for (w1, w2), freq in tqdm(edgeworth_bigram_freq.items()):\n",
    "    bigram_conditional_freq[w1][w2] = freq / total_counts_for_w1[w1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8197"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bigram_conditional_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_counts_for_w1[\"ruins\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float,\n",
       "            {'castle': 0.08333333333333333,\n",
       "             'old': 0.08333333333333333,\n",
       "             'farthest': 0.08333333333333333,\n",
       "             'could': 0.08333333333333333,\n",
       "             'improbability': 0.08333333333333333,\n",
       "             'rossmore': 0.08333333333333333,\n",
       "             'ancient': 0.08333333333333333,\n",
       "             'palaces': 0.08333333333333333,\n",
       "             'therefore': 0.08333333333333333,\n",
       "             'herculaneum': 0.16666666666666666,\n",
       "             'try': 0.08333333333333333})"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_conditional_freq[\"ruins\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0.08333333333333333)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edgeworth_bigram_freq[(\"ruins\", \"castle\")], bigram_conditional_freq[\"ruins\"][\"castle\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 0.16666666666666666)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edgeworth_bigram_freq[(\"ruins\", \"herculaneum\")], bigram_conditional_freq[\"ruins\"][\"herculaneum\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create trigrams, and calculate frequency, conditional frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgeworth_triram_freq = Counter(trigrams(clean_edgeworth_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('parent', 'assistant', 'maria'), 1),\n",
       " (('assistant', 'maria', 'edgeworth'), 1),\n",
       " (('maria', 'edgeworth', 'orphans'), 1),\n",
       " (('edgeworth', 'orphans', 'near'), 1),\n",
       " (('orphans', 'near', 'ruins'), 1),\n",
       " (('near', 'ruins', 'castle'), 1),\n",
       " (('ruins', 'castle', 'rossmore'), 1),\n",
       " (('castle', 'rossmore', 'ireland'), 1),\n",
       " (('rossmore', 'ireland', 'small'), 1),\n",
       " (('ireland', 'small', 'cabin'), 1)]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(edgeworth_triram_freq.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_counts_for_w1_w2 = defaultdict(int)\n",
    "for (w1, w2, w3), freq in edgeworth_triram_freq.items():\n",
    "    total_counts_for_w1_w2[(w1, w2)] += freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('parent', 'assistant'), 1),\n",
       " (('assistant', 'maria'), 1),\n",
       " (('maria', 'edgeworth'), 1),\n",
       " (('edgeworth', 'orphans'), 1),\n",
       " (('orphans', 'near'), 1),\n",
       " (('near', 'ruins'), 1),\n",
       " (('ruins', 'castle'), 1),\n",
       " (('castle', 'rossmore'), 4),\n",
       " (('rossmore', 'ireland'), 1),\n",
       " (('ireland', 'small'), 1)]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(total_counts_for_w1_w2.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 76702/76702 [00:00<00:00, 433093.09it/s]\n"
     ]
    }
   ],
   "source": [
    "trigram_conditional_freq = defaultdict(lambda: defaultdict(lambda: defaultdict(float)))\n",
    "for (w1, w2, w3), freq in tqdm(edgeworth_triram_freq.items()):\n",
    "    trigram_conditional_freq[w1][w2][w3] = freq / total_counts_for_w1_w2[(w1, w2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('parent', 'assistant', 'maria'): 1.0\n",
      "('assistant', 'maria', 'edgeworth'): 1.0\n",
      "('maria', 'edgeworth', 'orphans'): 1.0\n",
      "('edgeworth', 'orphans', 'near'): 1.0\n",
      "('orphans', 'near', 'ruins'): 1.0\n",
      "('orphans', 'soon', 'one'): 1.0\n",
      "('orphans', 'left', 'alone'): 1.0\n",
      "('orphans', 'removed', 'taking'): 1.0\n",
      "('orphans', 'ready', 'help'): 1.0\n",
      "('orphans', 'putting', 'garland'): 1.0\n"
     ]
    }
   ],
   "source": [
    "def print_trigram_conditional_freq(trigram_conditional_freq, head=10):\n",
    "    counter = 0\n",
    "    for w1, w2_dicts in trigram_conditional_freq.items():\n",
    "        for w2, w3_dict in w2_dicts.items():\n",
    "            for w3, freq in w3_dict.items():\n",
    "                print(f\"('{w1}', '{w2}', '{w3}'): {freq}\")\n",
    "                counter += 1\n",
    "                if counter == head:\n",
    "                    return\n",
    "\n",
    "print_trigram_conditional_freq(trigram_conditional_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate text by bigrams and conditional freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_model = {}\n",
    "for w1, w2_freq in iter(bigram_conditional_freq.items()):\n",
    "    if w1 not in bigrams_model:\n",
    "        bigrams_model[w1] = []\n",
    "    for w2, freq in w2_freq.items():\n",
    "        bigrams_model[w1].append((w2, freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: assistant maria edgeworth orphans near till morning\n"
     ]
    }
   ],
   "source": [
    "def generate_sentence_bigram(bigrams_model, start_seed, length, debug=False):\n",
    "    if start_seed not in bigrams_model:\n",
    "        raise ValueError(\"The start seed is not in the trigram model.\")\n",
    "    \n",
    "    current_word = start_seed\n",
    "    generated_text = [current_word]\n",
    "    \n",
    "    for _ in range(length-2):\n",
    "        if current_word in bigrams_model:\n",
    "            possible_continuations = bigrams_model[current_word]\n",
    "            words = [word for word, _ in possible_continuations]\n",
    "            weights = [probability for _, probability in possible_continuations]\n",
    "            assert len(words) == len(weights), \"Words and weights length must be same\"\n",
    "            # next_word = random.choice(possible_continuations)[0]\n",
    "            new_weighted_word = random.choices(words, weights, k=1)[0]\n",
    "            # print(\"|\" * 26)\n",
    "            # print(sorted(possible_continuations, key=lambda x: x[1], reverse=False))\n",
    "            # print(sorted(possible_continuations, key=lambda x: x[1], reverse=True))\n",
    "            # print(\"|\" * 26)\n",
    "            generated_text.append(new_weighted_word)\n",
    "            current_word = generated_text[-1]\n",
    "            if debug:\n",
    "                print(\"Possible continuations:\", len(possible_continuations))\n",
    "                print(possible_continuations)\n",
    "                print(\"Next word:\", new_weighted_word)\n",
    "                print(\"Generated text:\", \" \".join(generated_text))\n",
    "                print(\"---\" * 26)\n",
    "        else:\n",
    "            print(\"No continuation available\")\n",
    "            break \n",
    "    \n",
    "    return ' '.join(generated_text)\n",
    "\n",
    "start_seed = ('assistant') \n",
    "generated_text = generate_sentence_bigram(bigrams_model, start_seed, 8, debug=False) \n",
    "print(\"Result:\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate text by trigrams with random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible continuations: 1\n",
      "[('edgeworth', 1)]\n",
      "Next word: edgeworth\n",
      "Generated text: assistant maria edgeworth\n",
      "New generated pair: maria edgeworth\n",
      "------------------------------------------------------------------------------\n",
      "Possible continuations: 1\n",
      "[('orphans', 1)]\n",
      "Next word: orphans\n",
      "Generated text: assistant maria edgeworth orphans\n",
      "New generated pair: edgeworth orphans\n",
      "------------------------------------------------------------------------------\n",
      "Possible continuations: 1\n",
      "[('near', 1)]\n",
      "Next word: near\n",
      "Generated text: assistant maria edgeworth orphans near\n",
      "New generated pair: orphans near\n",
      "------------------------------------------------------------------------------\n",
      "Result: assistant maria edgeworth orphans near\n"
     ]
    }
   ],
   "source": [
    "trigrams_model = {}\n",
    "for (w1, w2, w3), freq in edgeworth_triram_freq.items():\n",
    "    if (w1, w2) not in trigrams_model:\n",
    "        trigrams_model[(w1, w2)] = []\n",
    "    trigrams_model[(w1, w2)].append((w3, freq))\n",
    "\n",
    "def generate_sentence_trigram(trigrams_model, start_seed, length, debug=False):\n",
    "    if start_seed not in trigrams_model:\n",
    "        raise ValueError(\"The start seed is not in the trigram model.\")\n",
    "    \n",
    "    current_pair = start_seed\n",
    "    generated_text = [current_pair[0], current_pair[1]]\n",
    "    \n",
    "    for _ in range(length-2):\n",
    "        if current_pair in trigrams_model:\n",
    "            possible_continuations = trigrams_model[current_pair]\n",
    "            \n",
    "            next_word = random.choice(possible_continuations)[0]\n",
    "            generated_text.append(next_word)\n",
    "            current_pair = (generated_text[-2], generated_text[-1])\n",
    "            if debug:\n",
    "                print(\"Possible continuations:\", len(possible_continuations))\n",
    "                print(possible_continuations)\n",
    "                print(\"Next word:\", next_word)\n",
    "                print(\"Generated text:\", \" \".join(generated_text))\n",
    "                print(\"New generated pair:\", generated_text[-2], generated_text[-1])\n",
    "                print(\"---\" * 26)\n",
    "        else:\n",
    "            print(\"No continuation available\")\n",
    "            break \n",
    "    \n",
    "    return ' '.join(generated_text)\n",
    "\n",
    "start_seed = ('assistant', 'maria') \n",
    "generated_text = generate_sentence_trigram(trigrams_model, start_seed, 5, debug=True) \n",
    "print(\"Result:\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and train Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def build_vocabulary(tokenized_training_text):\n",
    "    vocabulary = set()\n",
    "    for token in tokenized_training_text:\n",
    "        vocabulary.add(token)\n",
    "    return list(vocabulary)\n",
    "\n",
    "\n",
    "tokenized_text = clean_edgeworth_tokens\n",
    "\n",
    "vocabulary = build_vocabulary(tokenized_text)\n",
    "word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_layer(word_idx, vocabulary_size):\n",
    "    x = torch.zeros(vocabulary_size)\n",
    "    x[word_idx] = 1.0\n",
    "    return x\n",
    "\n",
    "\n",
    "def create_idx_pairs(tokens, word2idx, window_size=1):\n",
    "    idx_pairs = []\n",
    "    for i in range(len(tokens)):\n",
    "        token = tokens[i]\n",
    "        for neighbor in tokens[max(i - window_size, 0):min(i + window_size + 1, len(tokens))]:\n",
    "            if neighbor != token:\n",
    "                idx_pairs.append((word2idx[token], word2idx[neighbor]))\n",
    "    return idx_pairs\n",
    "\n",
    "idx_pairs = create_idx_pairs(clean_edgeworth_tokens, word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155290"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idx_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)  # Embedding layer\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)  # Prediction layer\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs)  # Convert words to embeddings\n",
    "        out = self.linear(embeds)  # Predict context words\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/155290 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 10159/155290 [01:14<17:40, 136.89it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m log_probs \u001b[38;5;241m=\u001b[39m model(center\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))  \u001b[38;5;66;03m# Center word needs to be wrapped in a tensor\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Compute the loss\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(log_probs, \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# Context word needs to be wrapped in a tensor\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Backward pass and optimize\u001b[39;00m\n\u001b[1;32m     29\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Model parameters\n",
    "vocab_size = len(vocabulary)\n",
    "embedding_dim = 100  # Adjust based on experimentation\n",
    "learning_rate = 0.001\n",
    "epochs = 50  # Adjust based on experimentation\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = SkipGramModel(vocab_size, embedding_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Convert idx_pairs to a tensor for more efficient processing\n",
    "idx_pairs_tensor = torch.tensor(idx_pairs, dtype=torch.long)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for center, context in tqdm(idx_pairs_tensor):\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        log_probs = model(center.unsqueeze(0))  # Center word needs to be wrapped in a tensor\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(log_probs, context.unsqueeze(0))  # Context word needs to be wrapped in a tensor\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # Print loss every epoch\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(idx_pairs)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakham = nltk.corpus.gutenberg.words('melville-moby_dick.txt')\n",
    "#filter from punctuation:\n",
    "shakham_filt = [token for token in shakham if token not in string.punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(260819, 221767)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Token count\n",
    "len(shakham), len(shakham_filt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "austen = nltk.corpus.gutenberg.words('austen-emma.txt')\n",
    "#filter from punctuation:\n",
    "austen_filt = [token for token in austen if token not in string.punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192427, 167028)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Token count\n",
    "len(austen), len(austen_filt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "whitman = nltk.corpus.gutenberg.words('whitman-leaves.txt')\n",
    "#filter from punctuation:\n",
    "whitman_filt = [token for token in whitman if token not in string.punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(154883, 127995)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Token count\n",
    "len(whitman), len(whitman_filt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kbtu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
